{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dad00fe",
   "metadata": {},
   "source": [
    "# Machine Learning #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf13bea1",
   "metadata": {},
   "source": [
    "## 基本术语"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640050a",
   "metadata": {},
   "source": [
    "1. Data set(数据集):\n",
    "\n",
    "    是一个关于sample, instance 的集合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2c57c",
   "metadata": {},
   "source": [
    "2. Instance(示例), Sample(样本):\n",
    "\n",
    "    是关于某一个对象的描述, 含有多种attribute(属性) 和attribute value(属性值)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce385d",
   "metadata": {},
   "source": [
    "3. attribute(属性), attribute value(属性值):\n",
    "    \n",
    "    是反映sample在某一方面的性质以及取值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20a37e",
   "metadata": {},
   "source": [
    "4. attribute space(属性空间), sample space(样本空间, 输入空间):\n",
    "    \n",
    "    某个sample的attributes张成的空间\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7417fee",
   "metadata": {},
   "source": [
    "5. dimensionality(维数):\n",
    "\n",
    "    sample 的attributes个数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d358149",
   "metadata": {},
   "source": [
    "6. hypothesis(假设), training set(训练集):\n",
    "\n",
    "    对于用来训练模型的数据称作training set, \n",
    "    对于sample set的train所得的关于数据的某种潜在的规律, 为逼近ground-truth(真相)叫假设"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d560f7",
   "metadata": {},
   "source": [
    "7. example(样例):\n",
    "    \n",
    "    对于sample, 加入label(标记)表示结果的整体"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22574fb2",
   "metadata": {},
   "source": [
    "8. label space(标记空间, 输出空间):\n",
    "    \n",
    "    sample 与 label相对应形成的examples所张成的空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f253d9c",
   "metadata": {},
   "source": [
    "9. classification(分类):\n",
    "\n",
    "    想要predict的值为离散的, 如0,1,2 称为classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16149108",
   "metadata": {},
   "source": [
    "10. regression(回归):\n",
    "\n",
    "    想要predict的值为连续值的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218d3a4",
   "metadata": {},
   "source": [
    "11. binary classification(二分类):\n",
    "\n",
    "    classification的值只有两个时称作binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47650f2",
   "metadata": {},
   "source": [
    "12. positive class(正类), negative class(负类, 反类):\n",
    "\n",
    "    binary classification中所设计的两个类分别称作此"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8b879",
   "metadata": {},
   "source": [
    "13. testing(测试), testing sample(测试样本):\n",
    "\n",
    "    得到predict模型后进行的测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74422a",
   "metadata": {},
   "source": [
    "14. clustering(聚类):\n",
    "    \n",
    "    将training set 中的数据分为若干组, 每组称作一个cluster(簇), 在clustering learning中, 将这些自动形成的cluster自动寻找特性并产生相应的label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304821d",
   "metadata": {},
   "source": [
    "15. supervised learning(监督学习), unsupervised learning(无监督学习):\n",
    "\n",
    "    classification 与 regression是supervised的代表, clustering是unsupervised的代表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b93a79",
   "metadata": {},
   "source": [
    "16. generalization(泛化):\n",
    "\n",
    "    学得模型适用于新的sample的能力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8cb3b",
   "metadata": {},
   "source": [
    "17. hypothesis space(假设空间), version space(版本空间):\n",
    "\n",
    "    潜在规律形成的hypothesis张成的空间叫hypothesis space, 从该空间中删除与正例不一致的, 或者与反例相同的hypothesis剩下的子空间称作version space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c42434",
   "metadata": {},
   "source": [
    "18. inductive bias(归纳偏好):\n",
    "\n",
    "    version space 中对于多个hypothesis均与label一致的情况, 需要采取何种hypothesis来确定某一个sample是否符合label, 称作inductive bias, 任何模型必须有bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec2ca4",
   "metadata": {},
   "source": [
    "19. No Free Lunch Theorem(NFT定理):\n",
    "\n",
    "    算法不论是什么样的, 期望值均相同, 必须针对相对应的具体问题选择具体的算法. \n",
    "    \n",
    "    有严格证明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58106f04",
   "metadata": {},
   "source": [
    "20. learner(学习器):\n",
    "\n",
    "    模型, 学习算法在给定数据和参数空间的实例化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea10408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcf254",
   "metadata": {},
   "source": [
    "## supervised监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29c7dc",
   "metadata": {},
   "source": [
    "* regression回归问题\n",
    "\n",
    "用于预测随着attribute变化而连续变化的取值label问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b3502",
   "metadata": {},
   "source": [
    "* classification分类问题\n",
    "\n",
    "用于预测随着attribute变化而变化的离散取值label问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97f2bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d03c2f",
   "metadata": {},
   "source": [
    "## unsupervised无监督学习 - 找到数据的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d497a46",
   "metadata": {},
   "source": [
    "* clustering聚类学习\n",
    "\n",
    "将attribute相对集中分布的samples的label分为一个cluster簇, 从而将dataset分为不同label's cluster的set, 预测新加入的sample属于哪一个cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263669e",
   "metadata": {},
   "source": [
    "## 模型评估与选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c24537",
   "metadata": {},
   "source": [
    "* overfitting(过拟合), underfitting(欠拟合):\n",
    "\n",
    "    overfitting是将模型训练太过, 将training set里面的独有特征学习当作所有sample均具有的attribute, 训练模型难以避免.\n",
    "    \n",
    "    underfitting是模型训练不够, 难以抓住sample的attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2afe2f",
   "metadata": {},
   "source": [
    "* empirical error(经验误差), generalization error(泛化误差):\n",
    "    \n",
    "    在training set里预测产生的误差叫empirical error 在新的samples中产生的误差叫generalization error.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea51a33",
   "metadata": {},
   "source": [
    " ###   模型评估常用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c2d09",
   "metadata": {},
   "source": [
    ">1. #### hold-out(留出法)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90348b",
   "metadata": {},
   "source": [
    "  将data set  $D$ 划分为两个互斥的集合, 一个用作training set  $S$ 另一个用作testing set  $T$ 且$D = S\\cup T$ , $S\\cap T = \\emptyset$, 在$S$中训练, 在$T$中测试, 用于评估generalization error. 将大约 $2/3\\sim 4/5$ 的data set用做训练, 剩余用做评估. 而training set最少要留出30个sample\n",
    "\n",
    "  另外为确保不受选取set随机性的影响, 要进行多次不同划分进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b24a0",
   "metadata": {},
   "source": [
    ">2. #### cross validation(交叉验证法)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac91f61",
   "metadata": {},
   "source": [
    "  将 $D$ 划分为 $k$ 个大小相似的互斥子集, 即$D = D_1\\cup D_2\\cup \\cdots \\cup D_k$ ,$D_i\\cap D_j = \\emptyset (i \\neq j)$. 每个子集均是从data set里面均匀得到(分层抽样), 使用 $k-1$ 个子集进行训练, 1个子集进行评估. 并轮流使用不同的 $D_k$ 作为评估子集, 一共进行$k$轮训练.\n",
    "\n",
    "* $ Leave-One-Out(LOO,留一法)$:\n",
    "\n",
    "任意训练子集$D_i$只含有一个sample, 即训练时训练 总数-1 的sample, 留下一个单独的sample用于评估, 并轮次进行 总数 次."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15340d",
   "metadata": {},
   "source": [
    ">3. #### bootstrapping(自助法)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93f34d",
   "metadata": {},
   "source": [
    "* 适用于data set 比较小的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63305cb1",
   "metadata": {},
   "source": [
    "  从给定的$m$个sample的$D$中每次随机复制一个sample到$D^{\\prime}$中, 重复执行$m$次. 其中, $D^{\\prime}$中的sample可能会出现多次, 也可能完全不出现, 完全不出现的概率为$${\\lim_{m\\to \\infty}}(1-\\frac {1}{m})^m = \\frac{1}{e}\\approx 0.368$$ 即有$36.8 \\% $的数据完全不会出现在$D^{\\prime}$中, 故我们可以将$D^{\\prime}$作为training set 而将$D-D^{\\prime}$作为评估用. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f4615",
   "metadata": {},
   "source": [
    "### 查全率,查准率,F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4707ba8",
   "metadata": {},
   "source": [
    " * binary classification中真实情况与预测情况的情形\n",
    "\n",
    ">|真实\\预测|正例|反例|\n",
    "|---|---|---|\n",
    "|正例|TP(真正例)|FN(假反例)|\n",
    "|反例|FP(假正例)|TN(真反例)|\n",
    "\n",
    " >以预测结果作为第二个PN选项, 预测为真就是P(positive), 假为N(negative), 再与真实情况比较, 如果与真实情况的真假相对应, 则为T(true), 相反为F(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a87e2",
   "metadata": {},
   "source": [
    "* 查全率, 查准率\n",
    "\n",
    "> 查准率(precision)定义为 $P = \\frac{TP}{TP+FP}$ 所有预测正例里面的真情况\n",
    "\n",
    "> 查全率(recall)定义为$R = \\frac{TP}{TP+FN}$ 所有真实正例里面的预测为正情况\n",
    "\n",
    "> 两者是相互矛盾的构象, 一个高另一个就低, 可以使用以recall为横轴, precision为纵轴的$P-R$图像. 以面积作为衡量标准.\n",
    "\n",
    ">* $Break-Event Point(BEP,平衡点)$\n",
    "\n",
    ">>面积难以计算时, 采用一条$y = x$直线分割图像, 此时直线意义为$precision = recall$, 以此直线与图像相交的点大小作为衡量标准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7c9a0",
   "metadata": {},
   "source": [
    "* $F1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee8e88",
   "metadata": {},
   "source": [
    ">$F1 = \\frac{2\\times P \\times R}{P + R} = \\frac{2\\times TP}{sample总数+TP-TN}$\n",
    "\n",
    ">$F1$是基于调和平均的, 原始定义为$\\frac{1}{F1} = \\frac{1}{2} \\cdot (\\frac{1}{P}+\\frac{1}{R})$\n",
    "\n",
    ">$F_{\\beta}$是$F1$加权调和平均, $\\frac{1}{F_{\\beta}} = \\frac{1}{2} \\cdot (\\frac{1}{P}+\\frac{\\beta^2}{R})$, 反解得到$$F_{\\beta} = \\frac{(1+\\beta^2)\\times P \\times R}{(\\beta^2\\times P) + R}$$实际使用中, $\\beta >1$时recall更重要, $\\beta  = 1$时退化为标准$F1$, $\\beta <1$时precision更重要\n",
    "\n",
    ">* $n$个binary classification混淆矩阵判断法\n",
    ">>* macro-$F1$(宏$F1$)\n",
    ">>>相当于将所有子binary classification的precision, recall 求平均得到 macro-$P$和macro-$R$后, 再计算macro-$F1$\n",
    ">>* micro-$F1$(微$F1$)\n",
    ">>>相当于先求TP,TN,FP,FN的平均, 再计算得到micro-$P$与micro-$R$, 然后计算micro-$F1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f8457",
   "metadata": {},
   "source": [
    "### ROC & AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a3bf3",
   "metadata": {},
   "source": [
    "* ROC(Receiver Operating Characteristic, 受试者工作特征)\n",
    ">根据学习器预测结果对样例进行排序, 按此计算True Positive Rate(真正例率)$TPR = \\frac{TP}{TP+FN}$(真实情况正例中,预测为真情况), 和False Positive Rate(假正例率)$FPR = \\frac{FP}{TN+FP}$(真实情况反例情况下, 预测为真情况). 以$TPR$为纵轴, $FPR$为横轴.得到ROC图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096bf5a7",
   "metadata": {},
   "source": [
    "* AUC(Area Under ROC Curve)\n",
    ">ROC曲线图中在ROC曲线以下部分的面积."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2da29",
   "metadata": {},
   "source": [
    "### 代价敏感错误率与代价曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7868f8",
   "metadata": {},
   "source": [
    "### 比较检验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73358fb8",
   "metadata": {},
   "source": [
    "### 假设检验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741928f",
   "metadata": {},
   "source": [
    "### 交叉验证t检验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873e22b",
   "metadata": {},
   "source": [
    "### McNemar检验\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c384067",
   "metadata": {},
   "source": [
    "### Friedman 与 Nemenyi后续检验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5e2a4",
   "metadata": {},
   "source": [
    "### 偏差与方差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c738bfc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abe202",
   "metadata": {},
   "source": [
    "## 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e30cd",
   "metadata": {},
   "source": [
    "将$d$个attributes描述的实例$\\textbf{x} = (x_1,x_2,\\cdots ,x_d)$通过一个对attributes的线性组合来预测的函数$$f(\\textbf{x}) =w_1x_1+w_2x_2+\\cdots +w_dx_d+b $$ 通过确定$\\textbf{W} = (w_1,w_2,\\cdots ,w_d)$与$b$即可确定模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c06dc",
   "metadata": {},
   "source": [
    "### 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227de663",
   "metadata": {},
   "source": [
    "使用均方误差衡量损失, 其中,$y$表示真实函数值, $X$表示数据集张量,其中$x_{ij}$中, $i$表示sample的序列(行数为sample个数),$j$为attribute序列(列数为attributes个数),$X$的最后一列全为1以便同$b$对应, 将$w$和$b$合并为同一组向量$\\hat w = (\\textbf{w},b)$,$X\\hat w$表示模型预测结果 则所需的参数就是使得均方误差最小的$\\hat w$, 均方误差:$$S = (y - X\\hat w)^T(y - X\\hat w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa842c7",
   "metadata": {},
   "source": [
    "要求最值, 则可对均方误差求梯度gradient并令其等于零, 即$$\\nabla S(\\hat w) = 2X^T(X\\hat w - y) == 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71c4c5",
   "metadata": {},
   "source": [
    "解之, 即得$$\\hat w^* = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752ff66",
   "metadata": {},
   "source": [
    "设预测时属性向量为$x$, 则最终预测结果为$$f(x) = x^T\\hat w^* = x^T(X^TX)^{-1}X^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b9c30",
   "metadata": {},
   "source": [
    "而现实中, $X^TX$可能非满秩, 即样本的attributes数量(列数)远多于样本数(行数), 故其解空间非零维, 则此时, 选择哪一个解向量作为输出, 取决于算法的归纳偏好, 常见的做法是引入regularization(正则化)项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ecb05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.40089155  3.03708158  1.25926826 -3.20078109]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.random(size=(4,3)) #创建input数组\n",
    "X = np.hstack((X,np.array([[1],[1],[1],[1]]))) #扩充X使之兼容(w,b)\n",
    "w = np.random.random(size=4) #创建w = (w,b)\n",
    "y = np.array([1,2,3,4]) #指定y\n",
    "\n",
    "# s = ((y-X.dot(w))**2).sum() #计算均方误差\n",
    "\n",
    "X = np.matrix(X) # 将X转化为矩阵形式, 便于求inv以及转置\n",
    "\n",
    "w_sol = ((X.T.dot(X)).I).dot(X.T).dot(y) #梯度求解, 得到均方误差损失下最佳的weight(或者使用@代替.dot)\n",
    "print (w_sol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358c146",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89f22c",
   "metadata": {},
   "source": [
    "## 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ace043",
   "metadata": {},
   "source": [
    "decision tree是一类常见的机器学习算法\n",
    "\n",
    "对数据集属性集$x$和label集$y$, 通过纯度算法, 从对于纯度贡献最大的属性开始决策, 最终得到一个决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac458c90",
   "metadata": {},
   "source": [
    "![decision tree](decision_tree.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from math import log\n",
    "import operator\n",
    " \n",
    " \n",
    " \n",
    "\"\"\"\n",
    "函数说明:创建测试数据集\n",
    "\"\"\"\n",
    "def createDataSet():\n",
    "    dataSet = [[0, 0, 0, 0, 'no'],         #数据集\n",
    "               [0, 0, 0, 1, 'no'],\n",
    "               [0, 1, 0, 1, 'yes'],\n",
    "               [0, 1, 1, 0, 'yes'],\n",
    "               [0, 0, 0, 0, 'no'],\n",
    "               [1, 0, 0, 0, 'no'],\n",
    "               [1, 0, 0, 1, 'no'],\n",
    "               [1, 1, 1, 1, 'yes'],\n",
    "               [1, 0, 1, 2, 'yes'],\n",
    "               [1, 0, 1, 2, 'yes'],\n",
    "               [2, 0, 1, 2, 'yes'],\n",
    "               [2, 0, 1, 1, 'yes'],\n",
    "               [2, 1, 0, 1, 'yes'],\n",
    "               [2, 1, 0, 2, 'yes'],\n",
    "               [2, 0, 0, 0, 'no']]\n",
    "    labels = ['年龄', '有工作', '有自己的房子', '信贷情况']        #分类属性\n",
    "    return dataSet, labels                           #返回数据集和分类属性\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "函数说明:计算给定数据集的经验熵(香农熵)\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    shannonEnt - 经验熵(香农熵)\n",
    "\"\"\"\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                 #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                          #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                   #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():   #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1               #Label计数\n",
    "    shannonEnt = 0.0                                 #经验熵(香农熵)\n",
    "    for key in labelCounts:                          #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires  #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "函数说明:按照给定特征划分数据集\n",
    "Parameters:\n",
    "    dataSet - 待划分的数据集\n",
    "    axis - 划分数据集的特征\n",
    "    value - 需要返回的特征的值\n",
    "\"\"\"\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []                                     #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]             #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                   #返回划分后的数据集\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "函数说明:选择最优特征\n",
    "Parameters:\n",
    "    dataSet - 数据集\n",
    "Returns:\n",
    "    bestFeature - 信息增益最大的(最优)特征的索引值\n",
    "\"\"\"\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                     #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                    #信息增益\n",
    "    bestFeature = -1                                      #最优特征的索引值\n",
    "    for i in range(numFeatures):                          #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                   #经验条件熵\n",
    "        for value in uniqueVals:                           #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)           #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)        #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                        #信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))             #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                              #计算信息增益\n",
    "            bestInfoGain = infoGain                                #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                        #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "函数说明:统计classList中出现此处最多的元素(类标签)\n",
    "Parameters:\n",
    "    classList - 类标签列表\n",
    "Returns:\n",
    "    sortedClassCount[0][0] - 出现此处最多的元素(类标签)\n",
    "\"\"\"\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:                                        #统计classList中每个元素出现的次数\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)        #根据字典的值降序排序\n",
    "    return sortedClassCount[0][0]                                #返回classList中出现次数最多的元素\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "函数说明:递归构建决策树\n",
    "Parameters:\n",
    "    dataSet - 训练数据集\n",
    "    labels - 分类属性标签\n",
    "    featLabels - 存储选择的最优特征标签\n",
    "Returns:\n",
    "    myTree - 决策树\n",
    "\"\"\"\n",
    "def createTree(dataSet, labels, featLabels):\n",
    "    classList = [example[-1] for example in dataSet]               #取分类标签(是否放贷:yes or no)\n",
    "    if classList.count(classList[0]) == len(classList):            #如果类别完全相同则停止继续划分\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:                                       #遍历完所有特征时返回出现次数最多的类标签\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)                   #选择最优特征\n",
    "    bestFeatLabel = labels[bestFeat]                               #最优特征的标签\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}                                    #根据最优特征的标签生成树\n",
    "    del(labels[bestFeat])                                          #删除已经使用特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]        #得到训练集中所有最优特征的属性值\n",
    "    uniqueVals = set(featValues)                                   #去掉重复的属性值\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:]\n",
    "        #递归调用函数createTree(),遍历特征，创建决策树。\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels, featLabels)\n",
    "    return myTree\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "函数说明:使用决策树执行分类\n",
    "Parameters:\n",
    "    inputTree - 已经生成的决策树\n",
    "    featLabels - 存储选择的最优特征标签\n",
    "    testVec - 测试数据列表，顺序对应最优特征标签\n",
    "Returns:\n",
    "    classLabel - 分类结果\n",
    "\"\"\"\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "    firstStr = next(iter(inputTree))             #获取决策树结点\n",
    "    secondDict = inputTree[firstStr]             #下一个字典\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    featLabels = []\n",
    "    myTree = createTree(dataSet, labels, featLabels)\n",
    "    print(myTree)\n",
    "    testVec = [0, 1]     # 测试数据\n",
    "    result = classify(myTree, featLabels, testVec)\n",
    "    if result == 'yes':\n",
    "        print('放贷')\n",
    "    if result == 'no':\n",
    "        print('不放贷')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb5092",
   "metadata": {},
   "source": [
    "此算法实质上是:\n",
    "\n",
    "计算所有属性的信息增益,找出最大的信息增益属性\n",
    "\n",
    "对于此属性的所有属性取值:\n",
    "\n",
    "{如果 在此属性取值下的数据集为空, 那么划分前的数据集中最多的label就是叶节点取值\n",
    "\n",
    "如果 不为空, 那么 在划分后的数据集中去掉这个属性, 在剩下的属性中选择信息增益最大的重复此过程}\n",
    "\n",
    "\n",
    ">---\n",
    "\n",
    "下列为进入递归后 递归停止情况:\n",
    "\n",
    "如果 在递归过程中, 存在一个属性取值, 在此属性取值下的数据集label均属于同一类, 则此叶节点取值就是这个label\n",
    "\n",
    "\n",
    "如果 递归过程中, 没有其他属性, 或者 在这些属性上, 所有样本取值均相同, 则在现在的数据集中label最多的那一类就是 叶节点取值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51890c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c406d8b",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5489e6",
   "metadata": {},
   "source": [
    "#### M-P神经元模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef0859",
   "metadata": {},
   "source": [
    "类似于一个函数, 接受很多的inputs 每一个input对应着一个权重weight, 这个神经元实质上是:\n",
    "\n",
    "一个激活函数$$f(\\sum_{i=1}^n w_ix_i - \\theta)$$ 他的自变量是$\\vec{w^T}\\vec x$与bias偏置 $\\theta$的线性组合, 得到一个输出传送给下一层神经元, 作为下一层神经元输入$\\vec x$其中的一个分量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7a411",
   "metadata": {},
   "source": [
    "#### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f04a6",
   "metadata": {},
   "source": [
    "* Sigmoid $$f(x) = \\frac {1}{1 + e^{-x}}$$具有极好的求导性质, $y^{\\prime} = y(1-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45058ccf",
   "metadata": {},
   "source": [
    "* ReLU $$$$\n",
    "分段函数, 在阈值$x_0$左边的值恒取值为零, 在右边的值输出原值, 一般阈值为0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0232c1fe",
   "metadata": {},
   "source": [
    "* $and\\ others$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cc9a3",
   "metadata": {},
   "source": [
    "#### Perceptron感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a51a72",
   "metadata": {},
   "source": [
    "由两层神经元组成, 一层为输入层, 一层为输出层, 输入层的neurons(神经元们)通过赋予一定权重的输出们$x_i$交给下一层神经元处理, 下一层神经元在此基础上通过激活函数产生自己的输出$y$, 下一层neuron在处理数据的时候还会有bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3806c7d",
   "metadata": {},
   "source": [
    "* 感知机权重的调整$$$$\n",
    "存在一个learning rate学习率$\\eta $, 由此来更新每个权重$w_i$ $$更新后的权重w_i = 原权重w_i + \\Delta w_i$$ $$\\Delta w_i = \\eta (y - \\hat y)x_i$$其中, $y$是label, $\\hat y$是感知机预测的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79f8a9",
   "metadata": {},
   "source": [
    "* Perceptron的局限$$$$\n",
    "只有两层的感知机没有办法处理非线性问题, 只能够使用一个超平面分割正例反例, 对于非线性划分没有办法. 可通过多层perceptron的组合形成hidden layer隐藏层来产生非线性分割"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c5b7c",
   "metadata": {},
   "source": [
    "#### error BackPropagation 误差反向传播算法(BP算法)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65612a92",
   "metadata": {},
   "source": [
    "BP算法基于gradient descent梯度下降策略, 对于以权重为自变量的损失函数$E = Loss(\\vec w)$, 要想找到Loss函数的最小值, 根据多元微分学, 梯度为零的驻点, 有可能是最小值点. 而沿着任意一点的梯度的反方向就是Loss下降最快的方向, 由此, 权重更新中的$$\\Delta w_i = -\\eta \\frac{\\partial E}{\\partial w_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89026e27",
   "metadata": {},
   "source": [
    "Loss函数对权重$w$的梯度, 可由神经元输出$\\hat y$, 对此神经元的输入$\\beta$组成链导法则:\n",
    "$$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial {\\hat y}}\\frac {\\partial {\\hat y}}{\\partial \\beta}\\frac {\\partial \\beta}{\\partial w}$$其中,设前一层神经元的输出为$b$,则$$\\frac {\\partial \\beta}{\\partial w} = b$$ $\\beta = \\vec w^T\\vec b$, 可由此链导法则的到精确的前一层神经元的每一个输出, 以用来调节这一层到更前一层的权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a399e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------调参前-----------\n",
      "参数: \n",
      "[[0.2, 0.6], [0.1, 0.2]]\n",
      "[0.2, 0.8]\n",
      "损失: \n",
      "[0.00970533]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------调参后-----------\n",
      "历经 4021 轮后:\n",
      "参数: \n",
      "[[0.16649323 0.57185484]\n",
      " [0.00672452 0.12164957]]\n",
      "[-0.21758047  0.44922848]\n",
      "损失: \n",
      "[0.00029984]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Network():\n",
    "\n",
    "    def __init__(self,input,w1,w2,output,learning_rate):\n",
    "        self.w1 = np.array(w1)\n",
    "        self.w2 = np.array(w2)\n",
    "        self.input = np.array(input) \n",
    "        self.output = np.array(output)\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def neuron(self,input): #指定神经元的激活函数\n",
    "        out = 1/(1 + np.exp(-1 * input))\n",
    "        return out\n",
    "\n",
    "    def forward(self): #前向算法, 只有三层, 输入-隐含-输出\n",
    "        to_hidden = self.w1 @ self.input\n",
    "        out_hidden = Network.neuron(self,to_hidden)\n",
    "        to_output = self.w2 @ out_hidden\n",
    "        output = Network.neuron(self,to_output)\n",
    "        return output\n",
    "\n",
    "    def delta_w2(self): #调参使用的从输出层到隐含层的\n",
    "        hat_y = Network.forward(self)\n",
    "        to_hidden = self.w1 @ self.input\n",
    "        beta = Network.neuron(self,to_hidden)\n",
    "        partial_E_w2 = (hat_y - self.output) * (1 - hat_y) * hat_y * beta\n",
    "        deltaw2 = -1 * self.lr * partial_E_w2\n",
    "        deltaw2 = np.reshape(deltaw2,2)\n",
    "        return deltaw2\n",
    "\n",
    "    def delta_w1(self): #调参使用的从隐含层到输入层的\n",
    "        partial_E_w2 = Network.delta_w2(self) * -1/self.lr\n",
    "        to_hidden = self.w1 @ self.input\n",
    "        beta = Network.neuron(self,to_hidden)\n",
    "        deltaw1 = -1 * self.lr * partial_E_w2 * beta * (1 - beta) * self.input\n",
    "        return deltaw1\n",
    "\n",
    "    def backward(self):#反向传播调参\n",
    "        self.w2 += Network.delta_w2(self)\n",
    "        self.w1 += Network.delta_w1(self)\n",
    "    \n",
    "    def loss(self):\n",
    "        hat_y = Network.forward(self)\n",
    "        loss = 0.5 * (hat_y - self.output)**2\n",
    "        return loss\n",
    "\n",
    "input = [[0.35],[0.9]]\n",
    "output = 0.5\n",
    "\n",
    "w1_o = [[0.2,0.6],[0.1,0.2]]\n",
    "w2_o = [0.2,0.8]\n",
    "\n",
    "net = Network(input,w1_o,w2_o,output,learning_rate=0.01)\n",
    "print('{:-^25}'.format('调参前'))\n",
    "print('参数: ')\n",
    "print(w1_o)\n",
    "print(w2_o)\n",
    "print('损失: ')\n",
    "print(net.loss())\n",
    "y = []\n",
    "while 1 :\n",
    "    net.backward()\n",
    "    y.append(net.loss())\n",
    "    if net.loss() <= 0.0003:\n",
    "        break\n",
    "x = range(len(y))\n",
    "plt.xlabel('times')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss-times')\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "print('{:-^25}'.format('调参后'))\n",
    "print(f'历经 {len(y)} 轮后:')\n",
    "print('参数: ')\n",
    "print(net.w1)\n",
    "print(net.w2)\n",
    "print('损失: ')\n",
    "print(net.loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5e779",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18d286",
   "metadata": {},
   "source": [
    "## Support Vector Machine支持向量机(SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33141afe",
   "metadata": {},
   "source": [
    "给定一个样本集, 其中属性维度有$m$维, 共有$d$个样本. label为$y$, 则此数据集可表示为$$D = \\{(\\vec x_1, y_1),(\\vec x_2,y_2),\\cdots (\\vec x_d, y_d)\\}$$其中, $\\vec x_i = [x_1,x_2,x_3,\\cdots ,x_m]$, 则通过机器学习, 有可能寻找到一个$m$维超平面(取决于数据集是否线性可分)$$\\vec w^T\\vec x + b = 0$$将数据集根据label的不同从而分到超平面的两侧, 其中$\\vec w = [w_1,w_2,\\cdots ,w_m]$为超平面的法向量, $b$为超平面距原点的距离."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc465bf",
   "metadata": {},
   "source": [
    "对于一个二维的划分而言:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27a8c6",
   "metadata": {},
   "source": [
    "![2D_SVM](2d_SVM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e783e4",
   "metadata": {},
   "source": [
    "$H$即为超平面, 样本空间内任意样本到超平面的距离为(类比于空间点到平面距离)$$r = \\frac{|\\vec w^T\\vec x + b|}{||\\vec w||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be6adc",
   "metadata": {},
   "source": [
    "support vector所在平面方程为$\\vec w^T \\vec x + b = \\pm c$, 若将决策超平面式子: $\\vec w^T \\vec x + b = 0$和support vector所在的边界超平面方程同时左右同时除以$c$, 则令$$\\frac {\\vec w}{c} = \\vec w^{\\prime}$$$$\\frac {b}{c} = b^{\\prime}$$可得到($y$为正负类取值)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e494ef",
   "metadata": {},
   "source": [
    "$$\\left\\{\n",
    "\\begin{aligned}\n",
    "\\vec w^{\\prime T}\\vec x + b^{\\prime} \\ge +1,& \\quad y = +1 \\\\\n",
    "\\vec w^{\\prime T}\\vec x + b^{\\prime} \\le -1,& \\quad y = -1 \n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62de38d",
   "metadata": {},
   "source": [
    "而在此替换后的结果上, 若将$\\vec w^{\\prime}$和$b^{\\prime}$替换为$w$, $b$仍不影响运算, 故将之全部替换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fa831",
   "metadata": {},
   "source": [
    "而在此边界上的样本, 使得上式的等号成立, 故将这些样本点分别称作Support Vector(支持向量)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846277f",
   "metadata": {},
   "source": [
    "两个不同的label的support vector到超平面距离之和为(因为其满足超平面上的点到这个扩张后的超平面距离中的$\\vec w^Tx + b = 0$): $$\\gamma = \\frac{2}{||\\vec w||}$$它被称作margin(间隔)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d2198",
   "metadata": {},
   "source": [
    "如果能够使得$\\gamma$最大, 那么超平面划分后的模型对于噪音的扰动抗干扰能力最强. 故欲找到具有maximum margin(最大间隔)的划分超平面, 则需要找到最小的$||\\vec w||$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c55839",
   "metadata": {},
   "source": [
    "故Support Vector Machine的基本型为: \n",
    "\n",
    "求解这样的一个$\\vec w$满足\n",
    "$$\\underset{\\vec w,b}{\\min} \\frac{1}{2}||\\vec w||^2$$\n",
    "$$s.t.\\quad y_i(\\vec w^T\\vec x_i +b) \\ge 1$$ $subject\\ to$后的式子表示label与超平面同号且相乘大于一, 满足前大括号式子, 其中, $\\vec x_i$, $y_i$为所有样本属性取值和label取值均有第二个式子成立"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564adb86",
   "metadata": {},
   "source": [
    "可以由此, 得到Lagrangrian函数为:$$L(\\vec w,b,\\lambda) = \\frac{1}{2}||\\vec w||^2 - \\sum_{i=1}^{d}\\lambda _i[y_i(\\vec w^T\\vec x_i+b)-1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cfab0",
   "metadata": {},
   "source": [
    "这个函数的KKT条件为:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "\\lambda \\ge 0 \\\\\n",
    "y_i(\\vec w^T\\vec x_i + b)-1 \\ge 0 \\\\\n",
    "\\lambda[y_i(\\vec w^T\\vec x_i + b)-1] = 0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadca917",
   "metadata": {},
   "source": [
    "作为support vector, 由于其在约束条件边界上, 故$\\lambda > 0$而$y_i(\\vec w^T \\vec x_i + b) - 1 = 0$即为$$y_if(\\vec x_i) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076f41e",
   "metadata": {},
   "source": [
    "随后利用凸函数对偶问题最优化求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df17709",
   "metadata": {},
   "source": [
    "#### Kernel Function核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e45f95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c026203",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837798ad",
   "metadata": {},
   "source": [
    "## 贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b82157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
